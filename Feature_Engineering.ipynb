{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory Questions"
      ],
      "metadata": {
        "id": "Uh4hYCVXD6WR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "  - A parameter is a value or variable used to define or modify a function, system, or process. Depending on the context, it can have slightly different meanings, but the core idea is the same: parameters provide input or constraints that help control the behavior of something.\n",
        "\n",
        "2. What is correlation?\n",
        "  - Correlation refers to a statistical relationship or association between two or more variables. When two variables are correlated, it means that changes in one variable are associated with changes in another. The correlation can indicate the direction (positive or negative) and the strength of this relationship.\n",
        "  What does negative correlation mean?\n",
        "  - Negative correlation means that as one variable increases, the other variable tends to decrease, or vice versa. Essentially, the two variables move in opposite directions.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "  - Machine Learning (ML) is a branch of artificial intelligence (AI) that involves the development of algorithms and statistical models that enable computers to improve their performance on tasks over time, based on experience or data, without being explicitly programmed. In other words, ML allows systems to learn from data, identify patterns, and make decisions or predictions.\n",
        "  - Main components of Machine Learning are:-\n",
        "  1. Data (training and testing)\n",
        "\n",
        "  2. Model (the mathematical representation)\n",
        "\n",
        "  3. Algorithm (the learning process)\n",
        "\n",
        "  4. Features (input variables)\n",
        "\n",
        "  5. Labels (desired outputs in supervised learning)\n",
        "\n",
        "  6. Training Process (learning from data)\n",
        "\n",
        "  7. Evaluation (assessing model performance)\n",
        "\n",
        "  8. Optimization (improving the model's performance)\n",
        "\n",
        "  9. Loss Function (measuring prediction error)\n",
        "\n",
        "  10. Inference (making predictions on new data)\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "  - The loss value is a crucial metric used to evaluate how well a machine learning model is performing. It quantifies the difference between the model's predictions and the actual values (or ground truth) from the data. By assessing the loss value, we can determine if the model is making accurate predictions or if there are errors in its outputs.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "  - In statistics and machine learning, variables are typically classified into two broad types: continuous variables and categorical variables. These two types of variables differ in how they represent data and how they are used in analysis or modeling.\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "  - In machine learning, categorical variables represent data that can take on a limited number of distinct values or categories (e.g., gender, color, product type). Since many machine learning algorithms work with numerical data, categorical variables need to be converted into a numerical format before they can be used for modeling.\n",
        "  - When it comes to handling categorical variables in machine learning, there are several common techniques that can be used to convert these variables into a numerical format suitable for model training.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "  - In machine learning, training and testing a dataset refer to the process of using a dataset to train a model and then evaluate its performance. These two phases are essential to build a machine learning model that generalizes well to new, unseen data.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "  - sklearn.preprocessing is a module in the Scikit-learn (sklearn) library, which provides a range of tools and functions for preprocessing data in machine learning. Preprocessing refers to the techniques applied to data before it is used for training a machine learning model, to ensure that the data is in an appropriate format and is ready for analysis. This step is essential because raw data is often in a form that is not directly usable by machine learning algorithms.\n",
        "\n",
        "  - The sklearn.preprocessing module includes several utilities for scaling, normalizing, encoding, and transforming data to prepare it for machine learning models.\n",
        "\n",
        "9. What is a Test set?\n",
        "  - A test set in machine learning refers to a subset of the dataset that is used to evaluate the performance of a trained model. It is a portion of the data that is kept separate from the training process and is used only after the model has been trained on the training set. The primary purpose of the test set is to assess how well the model generalizes to new, unseen data â€” that is, data it has not encountered during training.\n",
        "  \n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "  - In Python, the most common way to split data into training and testing sets for model fitting is by using the train_test_split function from the Scikit-learn library. This function randomly splits your dataset into two parts: one for training the model and one for testing it.\n",
        "  How do you approach a Machine Learning problem?\n",
        "  - Approaching a machine learning problem involves several key steps, which help ensure that the problem is tackled systematically and efficiently.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "  - Performing Exploratory Data Analysis (EDA) before fitting a model to the data is a crucial step in the machine learning workflow. It helps you better understand the dataset and the relationships between variables, identify potential issues, and make informed decisions that improve the quality and effectiveness of your machine learning model.\n",
        "\n",
        "12. What is correlation?\n",
        "  - Correlation refers to a statistical relationship between two or more variables. It measures the strength and direction of the linear relationship between them. In other words, correlation helps to determine whether and how strongly pairs of variables are related.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "  - A negative correlation means that two variables have an inverse relationship, meaning that as one variable increases, the other variable tends to decrease, and vice versa. In other words, when one variable goes up, the other goes down in a predictable manner.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "  - In Python, you can calculate the correlation between variables using several methods, with Pandas and NumPy being the most commonly used libraries for this task.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "  - Causation refers to a cause-and-effect relationship between two variables, where one variable (the cause) directly influences or brings about a change in the other variable (the effect). In other words, causation means that one event or variable is responsible for producing the outcome of another.\n",
        "\n",
        "  - Correlation means two things happen together, while causation means one thing directly causes another. For example, ice cream sales and crime rates might be correlated (both increase in summer), but that doesn't mean ice cream causes crime; both are influenced by a third factor: hot weather.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "  - An optimizer in machine learning is an algorithm or technique used to adjust the parameters (such as weights in a neural network) of a model to minimize the loss function or cost function. The goal of optimization is to find the best parameters that allow the model to make the most accurate predictions by reducing the error.\n",
        "  - Optimizers are algorithms that update model parameters (like weights and biases) to minimize a loss function during training. Here are some common types, with examples: Gradient Descent, Stochastic Gradient Descent (SGD), Mini-batch SGD, Adam, Adagrad, RMSprop, and AdaDelta.\n",
        "\n",
        "  1. Gradient Descent:\n",
        "    Explanation: A fundamental optimization algorithm that iteratively adjusts model parameters in the direction of the negative gradient of the loss function.\n",
        "    Example: Imagine you are trying to find the lowest point in a valley. Gradient descent would take steps in the direction of the steepest descent from your current position.\n",
        "\n",
        "  2. Stochastic Gradient Descent (SGD):\n",
        "    Explanation: An extension of gradient descent that updates parameters using the gradient of a single training example at a time, making it computationally faster but potentially noisier.\n",
        "    Example: Instead of calculating the average gradient across the entire dataset, SGD calculates the gradient for each data point individually and updates the parameters accordingly.\n",
        "\n",
        "  3. Mini-batch Gradient Descent:\n",
        "    Explanation: A compromise between batch gradient descent and SGD, updating parameters using the gradient of a small batch of training examples.\n",
        "    Example: Instead of using all the data or a single data point, mini-batch SGD uses a batch of 32, 64, or 128 data points to calculate the gradient and update the parameters.\n",
        "\n",
        "  4. Adam (Adaptive Moment Estimation):\n",
        "    Explanation: Combines the advantages of RMSprop and SGD with momentum, adapting the learning rate for each parameter based on the first and second moments of the gradients.\n",
        "    Example: Adam is a popular choice for many deep learning tasks because it often converges faster and more stably than other optimizers.\n",
        "\n",
        "  5. Adagrad (Adaptive Gradient):\n",
        "    Explanation: Adapts the learning rate for each parameter based on the historical gradients, giving smaller updates to frequently occurring parameters and larger updates to infrequent ones.\n",
        "    Example: Adagrad is particularly useful for sparse data, where some features are much more frequent than others.\n",
        "\n",
        "  6. RMSprop (Root Mean Square Propagation):\n",
        "    Explanation: Similar to Adagrad, but uses a moving average of the squared gradients to adapt the learning rate, which can help prevent the learning rate from decaying too quickly.\n",
        "    Example: RMSprop is often used in recurrent neural networks (RNNs) and other tasks where the gradients can be noisy or have a high variance.\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        "  - In scikit-learn, sklearn.linear_model is a module that provides a variety of linear models for regression, classification, and other machine learning tasks. Linear models are a class of algorithms that assume a linear relationship between input features and the target output. These models are particularly simple but effective for many machine learning problems.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "  - The model.fit() method in scikit-learn is used to train a machine learning model on a given dataset. When you call model.fit(), the algorithm learns the relationships between the input features (independent variables) and the target output (dependent variable), allowing it to make predictions on new, unseen data.\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "  - The model.predict() method in scikit-learn is used to make predictions after a model has been trained using the model.fit() method. When you call model.predict(), it takes the trained model and uses it to generate predictions based on new input data (features) that the model has not yet seen.\n",
        "  - The primary argument for model.predict() is:\n",
        "\n",
        "      X (Input Features): This is a 2D array or DataFrame containing the new data points (features) for which you want to make predictions. The shape of X must be the same as the input data used for training (i.e., it must have the same number of features).\n",
        "\n",
        "      Shape: (n_samples, n_features)\n",
        "\n",
        "      X contains the input data for which the model will predict the corresponding target values.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "  - In statistics, a continuous variable can take any value within a given range, while a categorical variable represents categories or groups, like gender or color.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "  - Feature scaling is a crucial data preprocessing technique in machine learning that transforms features to a common scale, ensuring all features contribute equally to the model's learning process and preventing those with larger scales from dominating. This helps improve model performance, convergence, and accuracy, especially for algorithms sensitive to feature magnitudes.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "  - In Python, scaling (or normalization) refers to the process of adjusting the values of numeric data so they are on a similar scale. This is especially important for machine learning algorithms that are sensitive to the scale of the input features, such as K-Nearest Neighbors (KNN), Support Vector Machines (SVMs), and Gradient Descent.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "  - sklearn.preprocessing is a module within the scikit-learn library that provides various utility functions and transformer classes to prepare raw data for machine learning models, including scaling, normalization, encoding, and handling missing values.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "  - To split data for model fitting in Python, use the train_test_split function from scikit-learn, specifying the dataset, test size, and optionally a random state for reproducibility.\n",
        "\n",
        "25. Explain data encoding?\n",
        "  - Data encoding is the process of converting information or data into a specific format for efficient storage, transmission, or processing, often for use in machine learning or digital communication."
      ],
      "metadata": {
        "id": "GzBFqqBlKIP_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ikrnV8T8hYz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}